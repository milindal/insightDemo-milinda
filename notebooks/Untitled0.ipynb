{
 "metadata": {
  "name": "",
  "signature": "sha256:b93a014fdb1f7961d390ed389bb716c4c2cb946165f993531ea5b389569e03c6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import operator\n",
      "import nltk\n",
      "import string\n",
      "\n",
      "def isPunct(word):\n",
      "  return len(word) == 1 and word in string.punctuation\n",
      "\n",
      "def isNumeric(word):\n",
      "  try:\n",
      "    float(word) if '.' in word else int(word)\n",
      "    return True\n",
      "  except ValueError:\n",
      "    return False\n",
      "\n",
      "class RakeKeywordExtractor:\n",
      "\n",
      "  def __init__(self):\n",
      "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
      "    self.top_fraction = 1 # consider top third candidate keywords by score\n",
      "\n",
      "  def _generate_candidate_keywords(self, sentences):\n",
      "    phrase_list = []\n",
      "    for sentence in sentences:\n",
      "      words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
      "        nltk.word_tokenize(sentence.lower()))\n",
      "      phrase = []\n",
      "      for word in words:\n",
      "        if word == \"|\" or isPunct(word):\n",
      "          if len(phrase) > 0:\n",
      "            phrase_list.append(phrase)\n",
      "            phrase = []\n",
      "        else:\n",
      "          phrase.append(word)\n",
      "    return phrase_list\n",
      "\n",
      "  def _calculate_word_scores(self, phrase_list):\n",
      "    word_freq = nltk.FreqDist()\n",
      "    word_degree = nltk.FreqDist()\n",
      "    for phrase in phrase_list:\n",
      "      degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
      "      for word in phrase:\n",
      "        word_freq[word] += 1\n",
      "        word_degree[word] +=  degree # other words\n",
      "    for word in word_freq.keys():\n",
      "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
      "    # word score = deg(w) / freq(w)\n",
      "    word_scores = {}\n",
      "    for word in word_freq.keys():\n",
      "      word_scores[word] = word_degree[word] / word_freq[word]\n",
      "    return word_scores\n",
      "\n",
      "  def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
      "    phrase_scores = {}\n",
      "    for phrase in phrase_list:\n",
      "      phrase_score = 0\n",
      "      for word in phrase:\n",
      "        phrase_score += word_scores[word]\n",
      "      phrase_scores[\" \".join(phrase)] = phrase_score\n",
      "    return phrase_scores\n",
      "    \n",
      "  def extract(self, text, incl_scores=False):\n",
      "    sentences = nltk.sent_tokenize(text)\n",
      "    phrase_list = self._generate_candidate_keywords(sentences)\n",
      "    word_scores = self._calculate_word_scores(phrase_list)\n",
      "    phrase_scores = self._calculate_phrase_scores(\n",
      "      phrase_list, word_scores)\n",
      "    sorted_phrase_scores = sorted(phrase_scores.iteritems(),\n",
      "      key=operator.itemgetter(1), reverse=True)\n",
      "    n_phrases = len(sorted_phrase_scores)\n",
      "    if incl_scores:\n",
      "      return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
      "    else:\n",
      "      return map(lambda x: x[0],\n",
      "        sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib2 import Request, urlopen, URLError\n",
      "import numpy as np \n",
      "import json \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import gensim as gs\n",
      "import nltk\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.manifold import TSNE\n",
      "import pickle\n",
      "import string \n",
      "import re\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions to clean the data\n",
      "def striphtml(data):\n",
      "    p = re.compile(r'<.*?>')\n",
      "    return p.sub('', data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stop_words = set(stopwords.words('english')).union(set(['``', \"''\", '.', ',', '!', ':', '[', ']', '...', \"\\'\\'\" , '']))\n",
      "\n",
      "# Stemming words (or not?)\n",
      "from nltk.stem import PorterStemmer\n",
      "ps = PorterStemmer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "database_dict = pickle.load(open('data/updated_database_dict.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the url list\n",
      "urls_list = [line.rstrip('\\n') for line in open('../insightDemo-milinda/data/URLStest.txt').readlines() if len(line) > 5] \n",
      "urls_list "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['http://www.nytimes.com/2015/09/11/us/politics/iran-nuclear-deal-senate.html',\n",
        " 'http://www.nytimes.com/2015/09/12/health/blood-pressure-study.html',\n",
        " 'http://www.nytimes.com/2012/06/26/us/supreme-court-rejects-part-of-arizona-immigration-law.html',\n",
        " 'http://www.nytimes.com/2015/06/15/opinion/workers-betrayed-by-visa-loopholes.html',\n",
        " 'http://www.nytimes.com/2015/09/15/world/europe/europe-migrants-germany.html',\n",
        " 'http://www.nytimes.com/2015/07/22/opinion/the-campaign-of-deception-against-planned-parenthood.html',\n",
        " 'http://www.nytimes.com/2015/06/05/opinion/edward-snowden-the-world-says-no-to-surveillance.html',\n",
        " 'http://www.nytimes.com/2015/09/20/opinion/sunday/a-toxic-work-world.html',\n",
        " 'http://www.nytimes.com/2015/08/16/technology/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html']"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_string = urls_list[-2]\n",
      "print url_string \n",
      "\n",
      "# Get the three most diverse trusted comments assosciated with the url string\n",
      "comments_df = database_dict[url_string]['comments_df']\n",
      "comment_bodies = comments_df['commentBody']\n",
      "\n",
      "# round 1: tokenizing, stemming, removing html tags and stop words\n",
      "comment_texts = [[ps.stem(filter(lambda x: x in string.printable, word).lower()) for word in nltk.word_tokenize(striphtml(comment)) \n",
      "              if word not in stop_words] for comment in comment_bodies]\n",
      "#     print len(comment_texts)\n",
      "\n",
      "# round 2: filtering, remove words from comments that are too short or which are not alphabets\n",
      "comment_texts = [ [word for word in comment if (len(word) > 2 and not word.isdigit()and word not in stop_words)] for comment in comment_texts]\n",
      "#     print comment_texts[0:5]\n",
      "\n",
      "# create a dictionary for the words that appear in the comments \n",
      "dictionary = gs.corpora.Dictionary(comment_texts)\n",
      "\n",
      "# Bag of words representation for the comments \n",
      "comment_corpus = [dictionary.doc2bow(text) for text in comment_texts]\n",
      "\n",
      "# Train tf-idf model on the comment_corpus\n",
      "tfidf = gs.models.TfidfModel(comment_corpus)\n",
      "corpus_tfidf = tfidf[comment_corpus]\n",
      "\n",
      "# initialize an LSI transformation\n",
      "lsi = gs.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=1000) \n",
      "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
      "#     lsi.print_topics(5)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://www.nytimes.com/2015/09/20/opinion/sunday/a-toxic-work-world.html\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the coordinates for the corpus to send into the Kmeans clustering algorithm\n",
      "comments_coords_lsi_ND = [] \n",
      "N_dimensions = 10\n",
      "for item in corpus_lsi: \n",
      "    coord_temp = [0]*N_dimensions\n",
      "    for val_tuple in item:\n",
      "        if val_tuple[0] < N_dimensions: \n",
      "            coord_temp[val_tuple[0]] = val_tuple[1]\n",
      "    comments_coords_lsi_ND.append(coord_temp)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_fixed = 3\n",
      "kmeans = KMeans(k_fixed).fit(np.array(comments_coords_lsi_ND))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lab = 0 \n",
      "comment_bodies_lab = comments_df.loc[kmeans.labels_ == lab, 'commentBody']\n",
      "print comment_bodies_lab.shape\n",
      "# print striphtml(comment_bodies_lab[0])\n",
      "all_comments_text = ''\n",
      "for comment in comment_bodies_lab: \n",
      "    all_comments_text += striphtml(comment) + ' ' \n",
      "rake = RakeKeywordExtractor()\n",
      "keywords = rake.extract(all_comments_text, incl_scores=True)\n",
      "print keywords[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(244,)\n",
        "[(u'elements mutually complement one another.our artificial bubble', 36.766666666666666), (u\"`` welfare '' stories cover `` low income working mothers\", 36.32479604265153), (u'mr. severgnini thinks americans could ever learn anything', 35.91117424242424)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys \n",
      "sys.path.insert(0, '/Users/milindal/Dropbox/Insight/rake_experiments/RAKE-tutorial')\n",
      "import rake \n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rake_object = rake.Rake(\"/Users/milindal/Dropbox/Insight/rake_experiments/RAKE-tutorial/SmartStoplist.txt\", 5, 3, 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for lab in range(3):\n",
      "    comment_bodies_lab = comments_df.loc[kmeans.labels_ == lab, 'commentBody']\n",
      "    print comment_bodies_lab.shape\n",
      "    # print striphtml(comment_bodies_lab[0])\n",
      "    all_comments_text = ''\n",
      "    for comment in comment_bodies_lab: \n",
      "        all_comments_text += striphtml(comment) + ' ' \n",
      "    keywords = rake_object.run(all_comments_text)\n",
      "    print \"Keywords:\", keywords[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(244,)\n",
        "Keywords:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [(u'child care', 4.567032967032967), (u'paid time', 4.52), (u'work world', 4.225225225225225)]\n",
        "(170,)\n",
        "Keywords:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [(u'human beings', 4.533333333333333), (u'good luck', 4.315384615384616), (u'life balance', 4.128333333333334)]\n",
        "(68,)\n",
        "Keywords:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [(u'work force', 4.217391304347826), (u'outpacing boys', 4.0), (u'high schools', 4.0)]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}