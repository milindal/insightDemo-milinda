{
 "metadata": {
  "name": "",
  "signature": "sha256:e699d129a53c5e69d1381d1ed65cfcade8e21d090cc70c049f49db6acc556ea4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib2 import Request, urlopen, URLError\n",
      "import numpy as np \n",
      "import json \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import gensim as gs\n",
      "import nltk\n",
      "from sklearn.cluster import KMeans\n",
      "import pickle\n",
      "import string \n",
      "import re\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper functions to clean the data\n",
      "def striphtml(data):\n",
      "    p = re.compile(r'<.*?>')\n",
      "    return p.sub('', data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "print stopwords.words('english')\n",
      "print stopwords.words('english').__class__.__name__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n",
        "list\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stop_words = set(stopwords.words('english'))\n",
      "print len(stop_words)\n",
      "print stop_words\n",
      "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
      "stop_words.update(['``', \"''\", '.', ',', '!', ':', '[', ']', '...', \"\\'\\'\" , '', 'would'])\n",
      "print len(stop_words)\n",
      "print stop_words\n",
      "# stop_words = set(stopwords.words('english')).union(set(['``', \"''\", '.', ',', '!', ':', '[', ']', '...', \"\\'\\'\" , '', 'would', 'the']))\n",
      "\n",
      "# Stemming words (or not?)\n",
      "from nltk.stem import PorterStemmer\n",
      "ps = PorterStemmer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "127\n",
        "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'herself', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u'does', u'above', u'between', u't', u'be', u'we', u'who', u'were', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'their', u'there', u'been', u'whom', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'these', u'up', u'will', u'below', u'can', u'theirs', u'my', u'and', u'then', u'is', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'yours', u'so', u'the', u'having', u'once'])\n",
        "146\n",
        "set(['', u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'herself', u'had', ',', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u'does', u'above', u'between', ';', '?', u't', u'be', u'we', u'who', u'were', u'here', u'hers', '[', u'by', u'on', u'about', 'would', u'of', u'against', u's', '(', '{', u'or', u'own', u'into', u'yourself', u'down', u'your', '\"', u'from', u'her', u'their', u'there', u'been', '.', u'whom', u'too', u'themselves', ':', u'was', u'until', u'more', '``', u'himself', u'that', u'but', '...', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'these', u'up', u'will', u'below', u'can', u'theirs', u'my', u'and', u'then', u'is', u'am', u'it', u'an', \"''\", u'as', u'itself', u'at', u'have', u'in', u'any', u'if', '!', u'again', u'no', ')', u'when', u'same', u'how', u'other', u'which', u'you', u'after', u'most', u'such', ']', u'why', u'a', u'off', \"'\", u'i', u'yours', u'so', u'the', '}', u'having', u'once'])\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "database_dict = pickle.load(open('data/updated_database_dict.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the url list\n",
      "urls_list = [line.rstrip('\\n') for line in open('data/URLStest.txt').readlines() if len(line) > 5] \n",
      "urls_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['http://www.nytimes.com/2015/09/11/us/politics/iran-nuclear-deal-senate.html',\n",
        " 'http://www.nytimes.com/2015/09/12/health/blood-pressure-study.html',\n",
        " 'http://www.nytimes.com/2012/06/26/us/supreme-court-rejects-part-of-arizona-immigration-law.html',\n",
        " 'http://www.nytimes.com/2015/06/15/opinion/workers-betrayed-by-visa-loopholes.html',\n",
        " 'http://www.nytimes.com/2015/09/15/world/europe/europe-migrants-germany.html',\n",
        " 'http://www.nytimes.com/2015/07/22/opinion/the-campaign-of-deception-against-planned-parenthood.html',\n",
        " 'http://www.nytimes.com/2015/06/05/opinion/edward-snowden-the-world-says-no-to-surveillance.html',\n",
        " 'http://www.nytimes.com/2015/09/20/opinion/sunday/a-toxic-work-world.html',\n",
        " 'http://www.nytimes.com/2015/08/16/technology/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_string = urls_list[-2]\n",
      "print url_string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://www.nytimes.com/2015/09/20/opinion/sunday/a-toxic-work-world.html\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the three most diverse trusted comments assosciated with the url string\n",
      "comments_df = database_dict[url_string]['comments_df']\n",
      "comment_bodies = comments_df['commentBody']\n",
      "\n",
      "# round 1: tokenizing, stemming, removing html tags and stop words\n",
      "# comment_texts = [[ps.stem(filter(lambda x: x in string.printable, word).lower()) for word in nltk.word_tokenize(striphtml(comment)) \n",
      "#               if word not in stop_words] for comment in comment_bodies]\n",
      "comment_texts = [[filter(lambda x: x in string.printable, word).lower() for word in nltk.word_tokenize(striphtml(comment)) \n",
      "                  if word not in stop_words] for comment in comment_bodies]\n",
      "#     print len(comment_texts)\n",
      "\n",
      "# round 2: filtering, remove words from comments that are too short or which are not alphabets\n",
      "comment_texts = [ [word for word in comment if (len(word) > 2 and not word.isdigit() and word not in stop_words)] for comment in comment_texts]\n",
      "#     print comment_texts[0:5]\n",
      "\n",
      "# create a dictionary for the words that appear in the comments \n",
      "dictionary = gs.corpora.Dictionary(comment_texts)\n",
      "\n",
      "# Bag of words representation for the comments \n",
      "comment_corpus = [dictionary.doc2bow(text) for text in comment_texts]\n",
      "\n",
      " # Train tf-idf model on the comment_corpus\n",
      "tfidf = gs.models.TfidfModel(comment_corpus)\n",
      "corpus_tfidf = tfidf[comment_corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in dictionary.iteritems(): \n",
      "    if v == 'the': \n",
      "        print 'What the'\n",
      "\n",
      "print ps.stem('worker')\n",
      "# for comment_list in comment_texts: \n",
      "#     print sum([1 for word in comment_list if word == 'the'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "worker\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Building an LDA model on the comment_corpus\n",
      "# setting fixed seed\n",
      "SOME_FIXED_SEED = 13\n",
      "np.random.seed(SOME_FIXED_SEED)\n",
      "Ntopics = 3\n",
      "lda = gs.models.ldamodel.LdaModel(comment_corpus, num_topics= Ntopics, id2word=dictionary, alpha='auto', passes = 10, iterations=100)\n",
      "lda_tf_idf = gs.models.ldamodel.LdaModel(corpus_tfidf, num_topics= Ntopics, id2word=dictionary, alpha='auto', passes = 10, iterations=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'topics from LDA on bag of words'\n",
      "lda.print_topics(Ntopics, num_words=30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topics from LDA on bag of words\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[u\"0.014*snowden + 0.007*government + 0.006*privacy + 0.006*edward + 0.006*mr. + 0.005*people + 0.005*freedom + 0.005*one + 0.005*world + 0.004*n't + 0.003*rights + 0.003*home + 0.003*thank + 0.003*surveillance + 0.003*like + 0.003*come + 0.003*back + 0.003*many + 0.003*right + 0.003*courage + 0.003*public + 0.003*country + 0.003*state + 0.003*russia + 0.002*good + 0.002*u.s. + 0.002*man + 0.002*time + 0.002*protect + 0.002*let\",\n",
        " u\"0.013*snowden + 0.011*government + 0.007*nsa + 0.006*data + 0.006*n't + 0.005*privacy + 0.005*surveillance + 0.004*one + 0.004*mr. + 0.004*even + 0.004*people + 0.003*citizens + 0.003*public + 0.003*private + 0.003*phone + 0.003*time + 0.003*information + 0.003*world + 0.003*like + 0.003*much + 0.003*security + 0.002*right + 0.002*collection + 0.002*edward + 0.002*want + 0.002*russia + 0.002*know + 0.002*freedom + 0.002*may + 0.002*make\",\n",
        " u\"0.026*snowden + 0.008*mr. + 0.008*n't + 0.007*government + 0.007*edward + 0.006*thank + 0.006*people + 0.005*hero + 0.005*country + 0.005*american + 0.004*privacy + 0.004*russia + 0.004*one + 0.004*surveillance + 0.004*obama + 0.004*citizens + 0.003*world + 0.003*right + 0.003*like + 0.003*information + 0.003*nsa + 0.003*many + 0.003*think + 0.003*get + 0.003*public + 0.003*pardon + 0.003*president + 0.003*know + 0.003*data + 0.003*hope\"]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'topics from LDA on tf-idf'\n",
      "lda_tf_idf.print_topics(Ntopics, num_words=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topics from LDA on tf-idf\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "[u\"0.001*work + 0.001*time + 0.001*america + 0.001*women + 0.001*n't + 0.001*workers + 0.001*people + 0.001*many + 0.001*way + 0.001*hours + 0.001*business + 0.001*article + 0.001*get + 0.001*men + 0.001*leave + 0.001*children + 0.001*culture + 0.001*need + 0.001*care + 0.001*much + 0.001*problem + 0.001*want + 0.001*another + 0.001*employees + 0.001*take + 0.001*make + 0.001*one + 0.001*families + 0.001*life + 0.001*less + 0.001*part + 0.001*like + 0.001*everyone + 0.001*things + 0.001*jobs + 0.001*also + 0.001*home + 0.001*working + 0.001*see + 0.001*keep + 0.001*family + 0.001*child + 0.001*change + 0.001*glad + 0.001*right + 0.001*wages + 0.001*values + 0.001*office + 0.001*pay + 0.001*conditions + 0.001*benefits + 0.001*long + 0.001*world + 0.001*'ve + 0.001*high + 0.001*going + 0.001*longer + 0.001*help + 0.001*issues + 0.001*everything + 0.001*paid + 0.001*employers + 0.001*unions + 0.001*seems + 0.001*union + 0.001*making + 0.001*income + 0.001*company + 0.001*best + 0.001*businesses + 0.001*could + 0.001*labor + 0.001*real + 0.001*point + 0.000*new + 0.000*'ll + 0.000*think + 0.000*corporate + 0.000*say + 0.000*bad + 0.000*stop + 0.000*even + 0.000*management + 0.000*workplace + 0.000*power + 0.000*money + 0.000*society + 0.000*cost + 0.000*countries + 0.000*benefit + 0.000*support + 0.000*demand + 0.000*someone + 0.000*government + 0.000*got + 0.000*end + 0.000*rather + 0.000*anything + 0.000*find + 0.000*parents\",\n",
        " u\"0.001*children + 0.001*women + 0.001*people + 0.001*n't + 0.001*one + 0.001*work + 0.001*care + 0.001*family + 0.001*think + 0.001*working + 0.001*men + 0.001*society + 0.001*need + 0.001*time + 0.001*kids + 0.001*job + 0.001*make + 0.001*world + 0.001*home + 0.001*balance + 0.001*many + 0.001*workplace + 0.001*want + 0.001*like + 0.001*life + 0.001*even + 0.001*get + 0.001*parents + 0.001*made + 0.001*child + 0.001*hours + 0.001*good + 0.001*'re + 0.001*much + 0.001*must + 0.001*ever + 0.001*take + 0.001*pay + 0.001*woman + 0.001*day + 0.001*companies + 0.001*two + 0.001*way + 0.001*workers + 0.001*country + 0.001*worked + 0.001*leave + 0.001*money + 0.001*career + 0.001*toxic + 0.001*families + 0.001*may + 0.001*business + 0.001*high + 0.001*could + 0.001*well + 0.001*social + 0.001*class + 0.001*jobs + 0.001*years + 0.001*quality + 0.000*different + 0.000*great + 0.000*little + 0.000*wealth + 0.000*idea + 0.000*company + 0.000*labor + 0.000*always + 0.000*someone + 0.000*employees + 0.000*change + 0.000*problem + 0.000*also + 0.000*getting + 0.000*going + 0.000*means + 0.000*office + 0.000*else + 0.000*every + 0.000*without + 0.000*find + 0.000*benefits + 0.000*see + 0.000*schools + 0.000*times + 0.000*let + 0.000*yes + 0.000*yet + 0.000*old + 0.000*weeks + 0.000*live + 0.000*americans + 0.000*end + 0.000*better + 0.000*choice + 0.000*still + 0.000*often + 0.000*support + 0.000*sick\",\n",
        " u\"0.001*women + 0.001*n't + 0.001*time + 0.001*job + 0.001*children + 0.001*work + 0.001*care + 0.001*hours + 0.001*people + 0.001*home + 0.001*kids + 0.001*workers + 0.001*one + 0.001*employers + 0.001*men + 0.001*need + 0.001*country + 0.001*working + 0.001*make + 0.001*day + 0.001*life + 0.001*young + 0.001*family + 0.001*parents + 0.001*american + 0.001*like + 0.001*never + 0.001*could + 0.001*many + 0.001*employees + 0.001*single + 0.001*companies + 0.001*child + 0.001*also + 0.001*problem + 0.001*even + 0.001*want + 0.001*know + 0.001*mom + 0.001*childless + 0.001*able + 0.001*back + 0.001*years + 0.001*better + 0.001*employer + 0.001*good + 0.001*someone + 0.001*come + 0.001*everyone + 0.001*corporations + 0.001*give + 0.001*human + 0.001*often + 0.001*week + 0.001*think + 0.001*really + 0.001*still + 0.001*company + 0.001*going + 0.001*way + 0.001*first + 0.001*business + 0.001*long + 0.001*well + 0.001*lives + 0.001*worked + 0.001*culture + 0.001*part + 0.001*employee + 0.001*pay + 0.001*support + 0.001*corporate + 0.001*sick + 0.001*might + 0.001*real + 0.001*get + 0.001*female + 0.001*leave + 0.001*career + 0.001*stay + 0.001*lot + 0.001*hard + 0.001*possible + 0.001*slaughter + 0.001*little + 0.001*grateful + 0.001*done + 0.001*rights + 0.001*longer + 0.001*needs + 0.001*part-time + 0.001*workplace + 0.001*change + 0.001*article + 0.001*start + 0.001*days + 0.000*every + 0.000*without + 0.000*money + 0.000*take\"]"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print comment_corpus.__class__.__name__\n",
      "Ntopics = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "list\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topic_labels = [] # array of a topic labels (same as kmeans labels), \n",
      "topic_clusters = {} # key: topic id, val: list of comment ids\n",
      "for idx in range(Ntopics): \n",
      "    topic_clusters[idx] = []\n",
      "for i, bow in enumerate(comment_corpus): \n",
      "    doc_topics = lda.get_document_topics(bow, minimum_probability=None)\n",
      "#     print 'document: %d' %i\n",
      "    scores = [score for (topic, score) in doc_topics]\n",
      "    max_score = max(scores)\n",
      "#     print 'max_score: ', max_score\n",
      "    max_score_topics = [topic for (topic, score) in doc_topics if score == max_score]\n",
      "#     print 'max_score_topics: ', max_score_topics\n",
      "    \n",
      "    # Assign the document to the topic with the max_topic_score\n",
      "    for topic in max_score_topics: \n",
      "        topic_clusters[topic].append(i)\n",
      "    \n",
      "    # labels: Assigning the document to the topic with the highest score\n",
      "    # In case there is more than one topic with the same max score, the \n",
      "    # document is assigned to the first topic with the highest score\n",
      "    topic_labels.append(max_score_topics[0])\n",
      "    \n",
      "#     for (topic, score) in doc_topics: \n",
      "#         print topic, score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(Ntopics): \n",
      "    print 'Number of documents assigned to topic %d: %d' %(i, len(topic_clusters[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of documents assigned to topic 0: 257\n",
        "Number of documents assigned to topic 1: 239\n",
        "Number of documents assigned to topic 2: 164\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the documents with the highest score \n",
      "comment_bodies[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "u'This article pits childfree and young people against the rest of the workers unnecessarily.   <br/><br/>It is unfair to expect the employer to decide what type of lifestyle is valuable giving greater benefits, leave and pay to certain workers.  This has already been going on way too long in America leading to the wage inequities that we see today.  There should be equal pay for equal work, no matter your lifestyle choices or circumstances. <br/><br/>Also, people do have choices:<br/>1) Some careers are much more demanding than others. People should choose their career carefully if they know they want to be a care-giver to children.  <br/><br/>2) Pick where you live with your goals in mind and your ability to live with financial stress.  You need two breadwinners in LA or NY to have the same life style as single breadwinner many other places. <br/><br/>This issue will not be solved by the employer, but by government policy, given the diversity of work places in america.  Thus, these decisions get made by voters through the people they elect to represent them.  I dont think the average american agrees with author given their choices in voting booth.  '"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comment_bodies[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "u\"I was really interested to read the results of that study of the consulting business, which finally matches what I observed at a top 10 law firm.  Sensible, smart, competent, and mentally balanced men AND women all left within 2-5 years.  The personal costs weren't worth it, and they had other more interesting things to do with their lives.  The people who remained were majority male because the mental imbalance required to be a BigLaw partner generally is more common in males: a low priority on personal relationships, a psychopathic level of ambition, an unwillingness to consider alternatives, an obsessive and narrow range of interests.  The firm lost all the most genuinely talented people, but they seemed pretty happy with the manic drones that remained.\""
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}